{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive CryoET Zarr Benchmark\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "- ðŸ“¥ Downloading real cryo-EM data from CryoET portal\n",
    "- ðŸ“Š 3D visualization with Napari and Vizarr\n",
    "- ðŸ”¬ Comprehensive benchmarking based on HEFTIE project\n",
    "- âœ… Image quality validation (SSIM, PSNR, MSE)\n",
    "- ðŸ“ˆ Performance analysis and recommendations\n",
    "\n",
    "## HEFTIE Benchmark Configurations\n",
    "\n",
    "Based on https://heftieproject.github.io/zarr-benchmarks/\n",
    "\n",
    "**Tested:**\n",
    "- Codecs: Blosc-Zstd, Blosc-LZ4, Blosc-Zlib, Zstd, GZip, No Compression\n",
    "- Shuffle: shuffle, bitshuffle, noshuffle\n",
    "- Compression levels: 1, 3, 5, 7, 9\n",
    "- Chunk sizes: 64, 128, 256\n",
    "\n",
    "**HEFTIE Recommendations:**\n",
    "- Level 3 is optimal baseline\n",
    "- Chunk size > 90 for best performance\n",
    "- Blosc-Zstd with shuffle for image data\n",
    "- Zstd for dense segmentation\n",
    "\n",
    "âš ï¸ **IMPORTANT**: Select kernel **\"Python 3.13 (zarr-benchmarks)\"** before running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Executable: {sys.executable}\")\n",
    "if \"3.13\" in sys.version and \"venv\" in sys.executable:\n",
    "    print(\"âœ“ Correct environment!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Please select 'Python 3.13 (zarr-benchmarks)' kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cryoet_data_portal import Client, Dataset\n",
    "import s3fs\n",
    "import zarr\n",
    "from zarr_benchmarks.read_write_zarr import read_write_zarr\n",
    "from zarr_benchmarks import utils\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your benchmark parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download size (max 512 recommended)\n",
    "DOWNLOAD_SIZE = 256  # Start smaller, increase to 512 if you want\n",
    "\n",
    "# Quick test or comprehensive?\n",
    "QUICK_MODE = True  # Set False for full HEFTIE benchmark suite\n",
    "\n",
    "if QUICK_MODE:\n",
    "    # Quick test: fewer configurations\n",
    "    CODECS = ['blosc_zstd', 'blosc_lz4', 'zstd', 'no_compression']\n",
    "    SHUFFLES = ['shuffle']  # Only test shuffle\n",
    "    LEVELS = [3, 5]  # Test HEFTIE baseline (3) and mid-high (5)\n",
    "    CHUNKS = [128]  # Single chunk size\n",
    "else:\n",
    "    # Full HEFTIE suite\n",
    "    CODECS = ['blosc_zstd', 'blosc_lz4', 'blosc_zlib', 'zstd', 'gzip', 'no_compression']\n",
    "    SHUFFLES = ['shuffle', 'bitshuffle', 'noshuffle']\n",
    "    LEVELS = [1, 3, 5, 7, 9]\n",
    "    CHUNKS = [64, 128, 256]\n",
    "\n",
    "ZARR_SPEC = 2  # Use v2 for vizarr compatibility\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Download size: {DOWNLOAD_SIZE}Â³ = {(DOWNLOAD_SIZE**3 * 4)/(1024**2):.1f} MB\")\n",
    "print(f\"  Mode: {'Quick' if QUICK_MODE else 'Comprehensive'}\")\n",
    "print(f\"  Codecs: {len(CODECS)}\")\n",
    "print(f\"  Tests: ~{len(CODECS) * len(LEVELS) * len(CHUNKS)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to CryoET Portal & Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to CryoET Data Portal...\")\n",
    "client = Client()\n",
    "dataset = Dataset.get_by_id(client, 10445)\n",
    "\n",
    "print(f\"âœ“ Dataset: {dataset.title}\")\n",
    "print(f\"  ID: {dataset.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a tomogram large enough\n",
    "print(f\"\\nFinding tomogram >= {DOWNLOAD_SIZE}Â³...\")\n",
    "\n",
    "runs = list(dataset.runs)\n",
    "selected_tomo = None\n",
    "selected_run = None\n",
    "\n",
    "for run in runs:\n",
    "    tomograms = list(run.tomograms)\n",
    "    for tomo in tomograms:\n",
    "        if (tomo.size_x >= DOWNLOAD_SIZE and \n",
    "            tomo.size_y >= DOWNLOAD_SIZE and \n",
    "            tomo.size_z >= DOWNLOAD_SIZE):\n",
    "            selected_tomo = tomo\n",
    "            selected_run = run\n",
    "            break\n",
    "    if selected_tomo:\n",
    "        break\n",
    "\n",
    "if not selected_tomo:\n",
    "    print(\"âš ï¸  Using largest available tomogram\")\n",
    "    max_vol = 0\n",
    "    for run in runs:\n",
    "        for tomo in list(run.tomograms):\n",
    "            vol = tomo.size_x * tomo.size_y * tomo.size_z\n",
    "            if vol > max_vol:\n",
    "                max_vol = vol\n",
    "                selected_tomo = tomo\n",
    "                selected_run = run\n",
    "\n",
    "print(f\"âœ“ Selected: {selected_tomo.name}\")\n",
    "print(f\"  Size: {selected_tomo.size_x} Ã— {selected_tomo.size_y} Ã— {selected_tomo.size_z}\")\n",
    "print(f\"  Voxel spacing: {selected_tomo.voxel_spacing} Ã…\")\n",
    "print(f\"  Run: {selected_run.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access S3 data\n",
    "print(\"\\nAccessing zarr data from S3...\")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "zarr_path = selected_tomo.s3_omezarr_dir.replace('s3://', '')\n",
    "store = s3fs.S3Map(root=zarr_path, s3=s3, check=False)\n",
    "zarr_group = zarr.open(store, mode='r')\n",
    "zarr_array = zarr_group['0']  # Full resolution\n",
    "\n",
    "print(f\"âœ“ Zarr shape: {zarr_array.shape}\")\n",
    "print(f\"  Dtype: {zarr_array.dtype}\")\n",
    "print(f\"  Chunks: {zarr_array.chunks}\")\n",
    "print(f\"  Compressor: Blosc-LZ4 (original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download centered cube\n",
    "actual_size = min(DOWNLOAD_SIZE, min(zarr_array.shape))\n",
    "\n",
    "z_c = zarr_array.shape[0] // 2\n",
    "y_c = zarr_array.shape[1] // 2\n",
    "x_c = zarr_array.shape[2] // 2\n",
    "\n",
    "z_start = max(0, z_c - actual_size // 2)\n",
    "z_end = min(zarr_array.shape[0], z_start + actual_size)\n",
    "y_start = max(0, y_c - actual_size // 2)\n",
    "y_end = min(zarr_array.shape[1], y_start + actual_size)\n",
    "x_start = max(0, x_c - actual_size // 2)\n",
    "x_end = min(zarr_array.shape[2], x_start + actual_size)\n",
    "\n",
    "print(f\"\\nDownloading {actual_size}Â³ cube from center...\")\n",
    "print(f\"Region: Z[{z_start}:{z_end}], Y[{y_start}:{y_end}], X[{x_start}:{x_end}]\")\n",
    "\n",
    "start_time = time.time()\n",
    "reference_data = np.array(zarr_array[z_start:z_end, y_start:y_end, x_start:x_end])\n",
    "download_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ Downloaded in {download_time:.2f}s\")\n",
    "print(f\"  Shape: {reference_data.shape}\")\n",
    "print(f\"  Size: {reference_data.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"  Range: [{reference_data.min():.3f}, {reference_data.max():.3f}]\")\n",
    "print(f\"  Mean: {reference_data.mean():.3f} Â± {reference_data.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Data (2D Slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slice visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle(f'CryoET Data: {selected_tomo.name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "mid_z = reference_data.shape[0] // 2\n",
    "mid_y = reference_data.shape[1] // 2\n",
    "mid_x = reference_data.shape[2] // 2\n",
    "\n",
    "# Three orthogonal slices\n",
    "im1 = axes[0, 0].imshow(reference_data[mid_z, :, :], cmap='gray')\n",
    "axes[0, 0].set_title(f'XY Slice (Z={mid_z})')\n",
    "axes[0, 0].axis('off')\n",
    "plt.colorbar(im1, ax=axes[0, 0], fraction=0.046)\n",
    "\n",
    "im2 = axes[0, 1].imshow(reference_data[:, mid_y, :], cmap='gray')\n",
    "axes[0, 1].set_title(f'XZ Slice (Y={mid_y})')\n",
    "axes[0, 1].axis('off')\n",
    "plt.colorbar(im2, ax=axes[0, 1], fraction=0.046)\n",
    "\n",
    "im3 = axes[0, 2].imshow(reference_data[:, :, mid_x], cmap='gray')\n",
    "axes[0, 2].set_title(f'YZ Slice (X={mid_x})')\n",
    "axes[0, 2].axis('off')\n",
    "plt.colorbar(im3, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "# Histogram\n",
    "axes[1, 0].hist(reference_data.flatten(), bins=100, color='steelblue', alpha=0.7)\n",
    "axes[1, 0].set_title('Intensity Distribution')\n",
    "axes[1, 0].set_xlabel('Intensity')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Statistics\n",
    "stats_text = f\"\"\"Shape: {reference_data.shape}\n",
    "Size: {reference_data.nbytes/(1024**2):.2f} MB\n",
    "Min: {reference_data.min():.3f}\n",
    "Max: {reference_data.max():.3f}\n",
    "Mean: {reference_data.mean():.3f}\n",
    "Std: {reference_data.std():.3f}\"\"\"\n",
    "axes[1, 1].text(0.1, 0.3, stats_text, fontsize=11, family='monospace')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Dataset info\n",
    "info_text = f\"\"\"Dataset: {dataset.id}\n",
    "Tomogram: {selected_tomo.name}\n",
    "Voxel spacing: {selected_tomo.voxel_spacing} Ã…\n",
    "Original: Blosc-LZ4\"\"\"\n",
    "axes[1, 2].text(0.1, 0.3, info_text, fontsize=11, family='monospace')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ 2D visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3D Visualization with Napari\n",
    "\n",
    "**Run this cell to open an interactive 3D viewer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: 3D visualization with napari\n",
    "# Uncomment to use (opens separate window)\n",
    "\n",
    "# import napari\n",
    "# viewer = napari.view_image(\n",
    "#     reference_data, \n",
    "#     name=f'{selected_tomo.name}',\n",
    "#     colormap='gray',\n",
    "#     contrast_limits=[reference_data.min(), reference_data.max()]\n",
    "# )\n",
    "# napari.run()\n",
    "\n",
    "print(\"ðŸ’¡ Uncomment the code above to view in 3D with napari\")\n",
    "print(\"   (Opens separate window for interactive exploration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Comprehensive Benchmarks\n",
    "\n",
    "This tests multiple compression configurations based on HEFTIE recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directory\n",
    "output_dir = pathlib.Path(\"data/output/comprehensive_benchmark\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store results\n",
    "all_results = []\n",
    "\n",
    "print(f\"Starting {'quick' if QUICK_MODE else 'comprehensive'} benchmark...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for image quality metrics\n",
    "def calculate_metrics(original, compressed):\n",
    "    \"\"\"Calculate SSIM, PSNR, MSE\"\"\"\n",
    "    try:\n",
    "        # Normalize for SSIM\n",
    "        orig_norm = (original - original.min()) / (original.max() - original.min() + 1e-10)\n",
    "        comp_norm = (compressed - compressed.min()) / (compressed.max() - compressed.min() + 1e-10)\n",
    "        \n",
    "        # Middle slice for SSIM (faster)\n",
    "        mid = original.shape[0] // 2\n",
    "        ssim_val = ssim(orig_norm[mid], comp_norm[mid], data_range=1.0)\n",
    "        \n",
    "        # Full volume for PSNR/MSE\n",
    "        data_range = original.max() - original.min()\n",
    "        psnr_val = psnr(original, compressed, data_range=data_range)\n",
    "        mse_val = mse(original, compressed)\n",
    "        \n",
    "        return ssim_val, psnr_val, mse_val\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Metrics failed - {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"âœ“ Metric function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configurations\n",
    "test_num = 0\n",
    "\n",
    "for codec in CODECS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {codec.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if codec == 'no_compression':\n",
    "        # No compression: test chunk sizes only\n",
    "        for chunk_size in CHUNKS:\n",
    "            test_num += 1\n",
    "            print(f\"[{test_num}] chunk={chunk_size}...\", end=' ')\n",
    "            \n",
    "            store_path = output_dir / f\"{codec}_c{chunk_size}.zarr\"\n",
    "            utils.remove_output_dir(store_path)\n",
    "            \n",
    "            chunks = (chunk_size, chunk_size, chunk_size)\n",
    "            \n",
    "            t0 = time.time()\n",
    "            read_write_zarr.write_zarr_array(\n",
    "                reference_data, store_path, overwrite=False,\n",
    "                chunks=chunks, compressor=None, zarr_spec=ZARR_SPEC\n",
    "            )\n",
    "            write_time = time.time() - t0\n",
    "            \n",
    "            t0 = time.time()\n",
    "            read_back = read_write_zarr.read_zarr_array(store_path)\n",
    "            read_time = time.time() - t0\n",
    "            \n",
    "            ratio = 1.0\n",
    "            size_mb = utils.get_directory_size(store_path) / (1024**2)\n",
    "            ssim_val, psnr_val, mse_val = calculate_metrics(reference_data, read_back)\n",
    "            \n",
    "            all_results.append({\n",
    "                'codec': codec,\n",
    "                'shuffle': 'N/A',\n",
    "                'level': 0,\n",
    "                'chunk_size': chunk_size,\n",
    "                'write_time': write_time,\n",
    "                'read_time': read_time,\n",
    "                'ratio': ratio,\n",
    "                'size_mb': size_mb,\n",
    "                'ssim': ssim_val,\n",
    "                'psnr': psnr_val,\n",
    "                'mse': mse_val\n",
    "            })\n",
    "            \n",
    "            print(f\"W:{write_time:.3f}s R:{read_time:.3f}s Size:{size_mb:.1f}MB\")\n",
    "    \n",
    "    elif 'blosc' in codec:\n",
    "        # Blosc variants: test shuffle + levels + chunks\n",
    "        cname = codec.split('_')[1]  # zstd, lz4, or zlib\n",
    "        \n",
    "        for shuffle in SHUFFLES:\n",
    "            for level in LEVELS:\n",
    "                for chunk_size in CHUNKS:\n",
    "                    test_num += 1\n",
    "                    print(f\"[{test_num}] {shuffle[:4]}, L{level}, c{chunk_size}...\", end=' ')\n",
    "                    \n",
    "                    store_path = output_dir / f\"{codec}_{shuffle}_l{level}_c{chunk_size}.zarr\"\n",
    "                    utils.remove_output_dir(store_path)\n",
    "                    \n",
    "                    chunks = (chunk_size, chunk_size, chunk_size)\n",
    "                    compressor = read_write_zarr.get_blosc_compressor(\n",
    "                        cname, level, shuffle, ZARR_SPEC\n",
    "                    )\n",
    "                    \n",
    "                    t0 = time.time()\n",
    "                    read_write_zarr.write_zarr_array(\n",
    "                        reference_data, store_path, overwrite=False,\n",
    "                        chunks=chunks, compressor=compressor, zarr_spec=ZARR_SPEC\n",
    "                    )\n",
    "                    write_time = time.time() - t0\n",
    "                    \n",
    "                    t0 = time.time()\n",
    "                    read_back = read_write_zarr.read_zarr_array(store_path)\n",
    "                    read_time = time.time() - t0\n",
    "                    \n",
    "                    ratio = read_write_zarr.get_compression_ratio(store_path)\n",
    "                    size_mb = utils.get_directory_size(store_path) / (1024**2)\n",
    "                    ssim_val, psnr_val, mse_val = calculate_metrics(reference_data, read_back)\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'codec': codec,\n",
    "                        'shuffle': shuffle,\n",
    "                        'level': level,\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'write_time': write_time,\n",
    "                        'read_time': read_time,\n",
    "                        'ratio': ratio,\n",
    "                        'size_mb': size_mb,\n",
    "                        'ssim': ssim_val,\n",
    "                        'psnr': psnr_val,\n",
    "                        'mse': mse_val\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"W:{write_time:.3f}s R:{read_time:.3f}s {ratio:.2f}x SSIM:{ssim_val:.4f if ssim_val else 'N/A'}\")\n",
    "    \n",
    "    else:\n",
    "        # Other codecs: test levels + chunks\n",
    "        for level in LEVELS:\n",
    "            for chunk_size in CHUNKS:\n",
    "                test_num += 1\n",
    "                print(f\"[{test_num}] L{level}, c{chunk_size}...\", end=' ')\n",
    "                \n",
    "                store_path = output_dir / f\"{codec}_l{level}_c{chunk_size}.zarr\"\n",
    "                utils.remove_output_dir(store_path)\n",
    "                \n",
    "                chunks = (chunk_size, chunk_size, chunk_size)\n",
    "                \n",
    "                if codec == 'zstd':\n",
    "                    compressor = read_write_zarr.get_zstd_compressor(level, ZARR_SPEC)\n",
    "                else:  # gzip\n",
    "                    compressor = read_write_zarr.get_gzip_compressor(level, ZARR_SPEC)\n",
    "                \n",
    "                t0 = time.time()\n",
    "                read_write_zarr.write_zarr_array(\n",
    "                    reference_data, store_path, overwrite=False,\n",
    "                    chunks=chunks, compressor=compressor, zarr_spec=ZARR_SPEC\n",
    "                )\n",
    "                write_time = time.time() - t0\n",
    "                \n",
    "                t0 = time.time()\n",
    "                read_back = read_write_zarr.read_zarr_array(store_path)\n",
    "                read_time = time.time() - t0\n",
    "                \n",
    "                ratio = read_write_zarr.get_compression_ratio(store_path)\n",
    "                size_mb = utils.get_directory_size(store_path) / (1024**2)\n",
    "                ssim_val, psnr_val, mse_val = calculate_metrics(reference_data, read_back)\n",
    "                \n",
    "                all_results.append({\n",
    "                    'codec': codec,\n",
    "                    'shuffle': 'N/A',\n",
    "                    'level': level,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'write_time': write_time,\n",
    "                    'read_time': read_time,\n",
    "                    'ratio': ratio,\n",
    "                    'size_mb': size_mb,\n",
    "                    'ssim': ssim_val,\n",
    "                    'psnr': psnr_val,\n",
    "                    'mse': mse_val\n",
    "                })\n",
    "                \n",
    "                print(f\"W:{write_time:.3f}s R:{read_time:.3f}s {ratio:.2f}x SSIM:{ssim_val:.4f if ssim_val else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nâœ“ Completed {len(all_results)} benchmark tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = output_dir / f\"benchmark_results_{DOWNLOAD_SIZE}cube.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ“ Saved results to: {csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nTotal configurations tested: {len(df)}\")\n",
    "print(f\"Data size: {reference_data.nbytes / (1024**2):.2f} MB\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Best Overall Performance:\")\n",
    "print(f\"   Fastest write: {df.loc[df['write_time'].idxmin(), 'codec']:20s} {df['write_time'].min():.3f}s\")\n",
    "print(f\"   Fastest read:  {df.loc[df['read_time'].idxmin(), 'codec']:20s} {df['read_time'].min():.3f}s\")\n",
    "print(f\"   Best compression: {df.loc[df['ratio'].idxmax(), 'codec']:17s} {df['ratio'].max():.2f}x\")\n",
    "print(f\"   Best SSIM:     {df.loc[df['ssim'].idxmax(), 'codec']:20s} {df['ssim'].max():.6f}\")\n",
    "print(f\"   Best PSNR:     {df.loc[df['psnr'].idxmax(), 'codec']:20s} {df['psnr'].max():.2f} dB\")\n",
    "\n",
    "print(\"\\n2. HEFTIE Recommended (level=3, shuffle, chunk>=90):\")\n",
    "heftie = df[(df['level']==3) & (df['shuffle'].isin(['shuffle','N/A'])) & (df['chunk_size']>=90)]\n",
    "if not heftie.empty:\n",
    "    best = heftie.loc[heftie['ratio'].idxmax()]\n",
    "    print(f\"   Codec: {best['codec']}\")\n",
    "    print(f\"   Compression: {best['ratio']:.2f}x\")\n",
    "    print(f\"   Write: {best['write_time']:.3f}s, Read: {best['read_time']:.3f}s\")\n",
    "    print(f\"   SSIM: {best['ssim']:.6f}, PSNR: {best['psnr']:.2f} dB\")\n",
    "\n",
    "print(\"\\n3. Comparison by Codec (level=5, chunk=128, shuffle where applicable):\")\n",
    "for codec in CODECS:\n",
    "    if codec == 'no_compression':\n",
    "        subset = df[(df['codec']==codec) & (df['chunk_size']==128)]\n",
    "    elif 'blosc' in codec:\n",
    "        subset = df[(df['codec']==codec) & (df['level']==5) & \n",
    "                    (df['chunk_size']==128) & (df['shuffle']=='shuffle')]\n",
    "    else:\n",
    "        subset = df[(df['codec']==codec) & (df['level']==5) & (df['chunk_size']==128)]\n",
    "    \n",
    "    if not subset.empty:\n",
    "        row = subset.iloc[0]\n",
    "        print(f\"   {codec:20s}: {row['ratio']:4.2f}x  W:{row['write_time']:5.3f}s  \"\n",
    "              f\"R:{row['read_time']:5.3f}s  SSIM:{row['ssim']:.4f if row['ssim'] else 'N/A':>6s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison (level=5, chunk=128)\n",
    "plot_df = df[(df['level'].isin([0,5])) & (df['chunk_size']==128) & \n",
    "             (df['shuffle'].isin(['shuffle','N/A']))].copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle(f'Compression Benchmark Results (Level 5, Chunk 128, Shuffle)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Write time\n",
    "axes[0, 0].bar(range(len(plot_df)), plot_df['write_time'], color='steelblue')\n",
    "axes[0, 0].set_xticks(range(len(plot_df)))\n",
    "axes[0, 0].set_xticklabels(plot_df['codec'], rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Time (s)')\n",
    "axes[0, 0].set_title('Write Performance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Read time\n",
    "axes[0, 1].bar(range(len(plot_df)), plot_df['read_time'], color='coral')\n",
    "axes[0, 1].set_xticks(range(len(plot_df)))\n",
    "axes[0, 1].set_xticklabels(plot_df['codec'], rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Time (s)')\n",
    "axes[0, 1].set_title('Read Performance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Compression ratio\n",
    "axes[0, 2].bar(range(len(plot_df)), plot_df['ratio'], color='green')\n",
    "axes[0, 2].set_xticks(range(len(plot_df)))\n",
    "axes[0, 2].set_xticklabels(plot_df['codec'], rotation=45, ha='right')\n",
    "axes[0, 2].set_ylabel('Ratio')\n",
    "axes[0, 2].set_title('Compression Ratio')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Storage size\n",
    "axes[1, 0].bar(range(len(plot_df)), plot_df['size_mb'], color='purple')\n",
    "axes[1, 0].set_xticks(range(len(plot_df)))\n",
    "axes[1, 0].set_xticklabels(plot_df['codec'], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Size (MB)')\n",
    "axes[1, 0].set_title('Storage Size')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# SSIM\n",
    "axes[1, 1].bar(range(len(plot_df)), plot_df['ssim'], color='orange')\n",
    "axes[1, 1].set_xticks(range(len(plot_df)))\n",
    "axes[1, 1].set_xticklabels(plot_df['codec'], rotation=45, ha='right')\n",
    "axes[1, 1].set_ylabel('SSIM')\n",
    "axes[1, 1].set_title('Structural Similarity (Higher=Better)')\n",
    "axes[1, 1].set_ylim([plot_df['ssim'].min()*0.9999, 1.0001])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PSNR\n",
    "axes[1, 2].bar(range(len(plot_df)), plot_df['psnr'], color='brown')\n",
    "axes[1, 2].set_xticks(range(len(plot_df)))\n",
    "axes[1, 2].set_xticklabels(plot_df['codec'], rotation=45, ha='right')\n",
    "axes[1, 2].set_ylabel('PSNR (dB)')\n",
    "axes[1, 2].set_title('Peak Signal-to-Noise Ratio')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / f'benchmark_comparison_{DOWNLOAD_SIZE}cube.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Comparison plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compression Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze compression levels for blosc_zstd (HEFTIE recommended)\n",
    "if 'blosc_zstd' in CODECS:\n",
    "    level_df = df[(df['codec']=='blosc_zstd') & (df['shuffle']=='shuffle') & (df['chunk_size']==128)]\n",
    "    \n",
    "    if not level_df.empty:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        fig.suptitle('Blosc-Zstd: Compression Level Analysis (shuffle, chunk=128)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Write time vs level\n",
    "        axes[0].plot(level_df['level'], level_df['write_time'], 'o-', linewidth=2, markersize=8)\n",
    "        axes[0].set_xlabel('Compression Level')\n",
    "        axes[0].set_ylabel('Write Time (s)')\n",
    "        axes[0].set_title('Write Performance')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].axvline(3, color='red', linestyle='--', alpha=0.5, label='HEFTIE baseline')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Compression ratio vs level\n",
    "        axes[1].plot(level_df['level'], level_df['ratio'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "        axes[1].set_xlabel('Compression Level')\n",
    "        axes[1].set_ylabel('Compression Ratio')\n",
    "        axes[1].set_title('Compression Efficiency')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].axvline(3, color='red', linestyle='--', alpha=0.5, label='HEFTIE baseline')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # SSIM vs level\n",
    "        axes[2].plot(level_df['level'], level_df['ssim'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "        axes[2].set_xlabel('Compression Level')\n",
    "        axes[2].set_ylabel('SSIM')\n",
    "        axes[2].set_title('Image Quality (SSIM)')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].axvline(3, color='red', linestyle='--', alpha=0.5, label='HEFTIE baseline')\n",
    "        axes[2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Level analysis complete\")\n",
    "        print(\"\\nðŸ’¡ HEFTIE finding: Level 3 provides optimal balance\")\n",
    "        print(\"   Higher levels give diminishing returns vs write time cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL RECOMMENDATIONS FOR CRYOET DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š Based on HEFTIE + This Benchmark:\")\n",
    "\n",
    "print(\"\\n1. For Archival Storage (Best Compression):\")\n",
    "best_comp = df.loc[df['ratio'].idxmax()]\n",
    "print(f\"   Codec: {best_comp['codec']}\")\n",
    "print(f\"   Settings: level={best_comp['level']}, shuffle={best_comp['shuffle']}, chunk={best_comp['chunk_size']}\")\n",
    "print(f\"   Compression: {best_comp['ratio']:.2f}x ({best_comp['size_mb']:.1f} MB)\")\n",
    "print(f\"   Quality: SSIM={best_comp['ssim']:.6f}, PSNR={best_comp['psnr']:.2f} dB\")\n",
    "\n",
    "print(\"\\n2. For Interactive Analysis (Best Balance):\")\n",
    "# Find best balance: good compression, fast read\n",
    "df['balance_score'] = df['ratio'] / df['read_time']  # Higher is better\n",
    "best_balance = df.loc[df['balance_score'].idxmax()]\n",
    "print(f\"   Codec: {best_balance['codec']}\")\n",
    "print(f\"   Settings: level={best_balance['level']}, shuffle={best_balance['shuffle']}, chunk={best_balance['chunk_size']}\")\n",
    "print(f\"   Read: {best_balance['read_time']:.3f}s, Compression: {best_balance['ratio']:.2f}x\")\n",
    "print(f\"   Quality: SSIM={best_balance['ssim']:.6f}\")\n",
    "\n",
    "print(\"\\n3. HEFTIE General Recommendations:\")\n",
    "print(\"   â€¢ Image data: Blosc-Zstd with shuffle\")\n",
    "print(\"   â€¢ Dense segmentation: Zstd\")\n",
    "print(\"   â€¢ Sparse segmentation: Blosc-Zstd\")\n",
    "print(\"   â€¢ Level 3: Optimal baseline\")\n",
    "print(\"   â€¢ Chunk size: Keep > 90 for best I/O performance\")\n",
    "print(\"   â€¢ Tensorstore > Zarr v3 > Zarr v2 (for speed)\")\n",
    "\n",
    "print(\"\\n4. Image Quality Validation:\")\n",
    "print(f\"   â€¢ All SSIM values: {df['ssim'].min():.6f} - {df['ssim'].max():.6f}\")\n",
    "print(f\"   â€¢ All PSNR values: {df['psnr'].min():.2f} - {df['psnr'].max():.2f} dB\")\n",
    "print(f\"   â€¢ Lossless compression maintains perfect quality (SSIM~1.0)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… COMPREHENSIVE BENCHMARK COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**To run more comprehensive tests:**\n",
    "1. Set `QUICK_MODE = False` at the top\n",
    "2. Increase `DOWNLOAD_SIZE` to 512 (takes longer)\n",
    "3. Re-run all cells\n",
    "\n",
    "**To visualize in 3D:**\n",
    "- Uncomment the napari cell above\n",
    "\n",
    "**To test with different datasets:**\n",
    "- Change dataset ID: `Dataset.get_by_id(client, OTHER_ID)`\n",
    "- Browse: https://cryoetdataportal.czscience.com/\n",
    "\n",
    "**Resources:**\n",
    "- HEFTIE Project: https://heftieproject.github.io/zarr-benchmarks/\n",
    "- CryoET Portal: https://cryoetdataportal.czscience.com/\n",
    "- Zarr Docs: https://zarr.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (zarr-benchmarks)",
   "language": "python",
   "name": "zarr-benchmarks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
