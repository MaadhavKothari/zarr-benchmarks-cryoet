{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Advanced Compression Benchmarking Matrix\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook extends the basic compression benchmarks with:\n",
    "- üî¨ **Additional codecs**: Blosc-Zlib, GZip levels\n",
    "- üìä **Comprehensive matrices**: Heat maps comparing all configurations\n",
    "- üéØ **Multi-dimensional analysis**: Level √ó Codec √ó Chunk size\n",
    "- üìà **Trade-off analysis**: Compression vs Speed vs Quality\n",
    "- üó∫Ô∏è **Interactive visualizations**: Color-coded performance matrices\n",
    "\n",
    "## Compression Methods Tested\n",
    "\n",
    "**Lossless Compression:**\n",
    "- Blosc-Zstd (shuffle, bitshuffle, noshuffle)\n",
    "- Blosc-LZ4 (shuffle, bitshuffle, noshuffle)\n",
    "- Blosc-Zlib (shuffle, bitshuffle, noshuffle)\n",
    "- Zstd (standalone)\n",
    "- GZip (multiple levels)\n",
    "- No compression (baseline)\n",
    "\n",
    "**Configuration Space:**\n",
    "- Compression levels: 1, 3, 5, 7, 9\n",
    "- Chunk sizes: 64¬≥, 128¬≥, 256¬≥\n",
    "- Shuffle options: shuffle, bitshuffle, noshuffle\n",
    "\n",
    "**Total combinations: ~150 configurations**\n",
    "\n",
    "‚ö†Ô∏è **Note**: This is a comprehensive benchmark that will take ~30-60 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cryoet_data_portal import Client, Dataset\n",
    "import s3fs\n",
    "import zarr\n",
    "from zarr_benchmarks.read_write_zarr import read_write_zarr\n",
    "from zarr_benchmarks import utils\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings to control the benchmark scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "DOWNLOAD_SIZE = 256  # Cube size to download\n",
    "\n",
    "# Benchmark scope\n",
    "FULL_MATRIX = False  # Set True for complete matrix (takes longer)\n",
    "\n",
    "if FULL_MATRIX:\n",
    "    # Full comprehensive matrix\n",
    "    CODECS = ['blosc_zstd', 'blosc_lz4', 'blosc_zlib', 'zstd', 'gzip', 'no_compression']\n",
    "    SHUFFLES = ['shuffle', 'bitshuffle', 'noshuffle']\n",
    "    LEVELS = [1, 3, 5, 7, 9]\n",
    "    CHUNKS = [64, 128, 256]\n",
    "else:\n",
    "    # Focused matrix (faster, still comprehensive)\n",
    "    CODECS = ['blosc_zstd', 'blosc_lz4', 'blosc_zlib', 'zstd', 'gzip', 'no_compression']\n",
    "    SHUFFLES = ['shuffle', 'noshuffle']\n",
    "    LEVELS = [1, 3, 5, 9]\n",
    "    CHUNKS = [64, 128, 256]\n",
    "\n",
    "ZARR_SPEC = 2\n",
    "\n",
    "# Calculate total tests\n",
    "blosc_tests = 3 * len(SHUFFLES) * len(LEVELS) * len(CHUNKS)  # 3 blosc variants\n",
    "other_tests = 2 * len(LEVELS) * len(CHUNKS)  # zstd, gzip\n",
    "baseline_tests = len(CHUNKS)  # no_compression\n",
    "total_tests = blosc_tests + other_tests + baseline_tests\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Download size: {DOWNLOAD_SIZE}¬≥ = {(DOWNLOAD_SIZE**3 * 4)/(1024**2):.1f} MB\")\n",
    "print(f\"  Mode: {'Full Matrix' if FULL_MATRIX else 'Focused Matrix'}\")\n",
    "print(f\"  Codecs: {len(CODECS)}\")\n",
    "print(f\"  Shuffles: {len(SHUFFLES)} (for Blosc)\")\n",
    "print(f\"  Levels: {len(LEVELS)}\")\n",
    "print(f\"  Chunk sizes: {len(CHUNKS)}\")\n",
    "print(f\"  Total tests: ~{total_tests}\")\n",
    "print(f\"  Estimated time: ~{total_tests * 0.5:.0f}-{total_tests * 1:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 1. Download CryoET Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to CryoET Data Portal...\")\n",
    "client = Client()\n",
    "dataset = Dataset.get_by_id(client, 10445)\n",
    "\n",
    "print(f\"‚úì Dataset: {dataset.title}\")\n",
    "\n",
    "# Find tomogram\n",
    "runs = list(dataset.runs)\n",
    "selected_tomo = None\n",
    "for run in runs:\n",
    "    for tomo in list(run.tomograms):\n",
    "        if (tomo.size_x >= DOWNLOAD_SIZE and \n",
    "            tomo.size_y >= DOWNLOAD_SIZE and \n",
    "            tomo.size_z >= DOWNLOAD_SIZE):\n",
    "            selected_tomo = tomo\n",
    "            break\n",
    "    if selected_tomo:\n",
    "        break\n",
    "\n",
    "print(f\"‚úì Selected: {selected_tomo.name}\")\n",
    "print(f\"  Size: {selected_tomo.size_x} √ó {selected_tomo.size_y} √ó {selected_tomo.size_z}\")\n",
    "\n",
    "# Download data\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "zarr_path = selected_tomo.s3_omezarr_dir.replace('s3://', '')\n",
    "store = s3fs.S3Map(root=zarr_path, s3=s3, check=False)\n",
    "zarr_group = zarr.open(store, mode='r')\n",
    "zarr_array = zarr_group['0']\n",
    "\n",
    "# Download centered cube\n",
    "actual_size = min(DOWNLOAD_SIZE, min(zarr_array.shape))\n",
    "z_c = zarr_array.shape[0] // 2\n",
    "y_c = zarr_array.shape[1] // 2\n",
    "x_c = zarr_array.shape[2] // 2\n",
    "\n",
    "z_start = max(0, z_c - actual_size // 2)\n",
    "z_end = min(zarr_array.shape[0], z_start + actual_size)\n",
    "y_start = max(0, y_c - actual_size // 2)\n",
    "y_end = min(zarr_array.shape[1], y_start + actual_size)\n",
    "x_start = max(0, x_c - actual_size // 2)\n",
    "x_end = min(zarr_array.shape[2], x_start + actual_size)\n",
    "\n",
    "print(f\"\\nDownloading {actual_size}¬≥ cube...\")\n",
    "start_time = time.time()\n",
    "reference_data = np.array(zarr_array[z_start:z_end, y_start:y_end, x_start:x_end])\n",
    "download_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Downloaded in {download_time:.2f}s\")\n",
    "print(f\"  Shape: {reference_data.shape}\")\n",
    "print(f\"  Size: {reference_data.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bench-header",
   "metadata": {},
   "source": [
    "## 2. Run Comprehensive Benchmark Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "output_dir = pathlib.Path(\"data/output/compression_matrix\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "def calculate_metrics(original, compressed):\n",
    "    \"\"\"Calculate image quality metrics\"\"\"\n",
    "    try:\n",
    "        orig_norm = (original - original.min()) / (original.max() - original.min() + 1e-10)\n",
    "        comp_norm = (compressed - compressed.min()) / (compressed.max() - compressed.min() + 1e-10)\n",
    "        mid = original.shape[0] // 2\n",
    "        ssim_val = ssim(orig_norm[mid], comp_norm[mid], data_range=1.0)\n",
    "        data_range = original.max() - original.min()\n",
    "        psnr_val = psnr(original, compressed, data_range=data_range)\n",
    "        mse_val = mse(original, compressed)\n",
    "        return ssim_val, psnr_val, mse_val\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive benchmark\n",
    "import sys\n",
    "\n",
    "test_num = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for codec in CODECS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing: {codec.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if codec == 'no_compression':\n",
    "        for chunk_size in CHUNKS:\n",
    "            test_num += 1\n",
    "            print(f\"[{test_num}/{total_tests}] chunk={chunk_size}...\", end=' ')\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            store_path = output_dir / f\"{codec}_c{chunk_size}.zarr\"\n",
    "            utils.remove_output_dir(store_path)\n",
    "            chunks = (chunk_size, chunk_size, chunk_size)\n",
    "            \n",
    "            t0 = time.time()\n",
    "            read_write_zarr.write_zarr_array(\n",
    "                reference_data, store_path, overwrite=False,\n",
    "                chunks=chunks, compressor=None, zarr_spec=ZARR_SPEC\n",
    "            )\n",
    "            write_time = time.time() - t0\n",
    "            \n",
    "            t0 = time.time()\n",
    "            read_back = read_write_zarr.read_zarr_array(store_path)\n",
    "            read_time = time.time() - t0\n",
    "            \n",
    "            ratio = 1.0\n",
    "            size_mb = utils.get_directory_size(store_path) / (1024**2)\n",
    "            ssim_val, psnr_val, mse_val = calculate_metrics(reference_data, read_back)\n",
    "            \n",
    "            all_results.append({\n",
    "                'codec': codec,\n",
    "                'shuffle': 'N/A',\n",
    "                'level': 0,\n",
    "                'chunk_size': chunk_size,\n",
    "                'write_time': write_time,\n",
    "                'read_time': read_time,\n",
    "                'ratio': ratio,\n",
    "                'size_mb': size_mb,\n",
    "                'ssim': ssim_val,\n",
    "                'psnr': psnr_val,\n",
    "                'mse': mse_val,\n",
    "                'throughput_write_mbs': (reference_data.nbytes / (1024**2)) / write_time,\n",
    "                'throughput_read_mbs': (reference_data.nbytes / (1024**2)) / read_time\n",
    "            })\n",
    "            \n",
    "            print(f\"W:{write_time:.3f}s R:{read_time:.3f}s\")\n",
    "    \n",
    "    elif 'blosc' in codec:\n",
    "        cname = codec.split('_')[1]\n",
    "        \n",
    "        for shuffle in SHUFFLES:\n",
    "            for level in LEVELS:\n",
    "                for chunk_size in CHUNKS:\n",
    "                    test_num += 1\n",
    "                    print(f\"[{test_num}/{total_tests}] {shuffle[:3]}, L{level}, c{chunk_size}...\", end=' ')\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    store_path = output_dir / f\"{codec}_{shuffle}_l{level}_c{chunk_size}.zarr\"\n",
    "                    utils.remove_output_dir(store_path)\n",
    "                    chunks = (chunk_size, chunk_size, chunk_size)\n",
    "                    compressor = read_write_zarr.get_blosc_compressor(cname, level, shuffle)\n",
    "                    \n",
    "                    t0 = time.time()\n",
    "                    read_write_zarr.write_zarr_array(\n",
    "                        reference_data, store_path, overwrite=False,\n",
    "                        chunks=chunks, compressor=compressor, zarr_spec=ZARR_SPEC\n",
    "                    )\n",
    "                    write_time = time.time() - t0\n",
    "                    \n",
    "                    t0 = time.time()\n",
    "                    read_back = read_write_zarr.read_zarr_array(store_path)\n",
    "                    read_time = time.time() - t0\n",
    "                    \n",
    "                    ratio = read_write_zarr.get_compression_ratio(store_path)\n",
    "                    size_mb = utils.get_directory_size(store_path) / (1024**2)\n",
    "                    ssim_val, psnr_val, mse_val = calculate_metrics(reference_data, read_back)\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'codec': codec,\n",
    "                        'shuffle': shuffle,\n",
    "                        'level': level,\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'write_time': write_time,\n",
    "                        'read_time': read_time,\n",
    "                        'ratio': ratio,\n",
    "                        'size_mb': size_mb,\n",
    "                        'ssim': ssim_val,\n",
    "                        'psnr': psnr_val,\n",
    "                        'mse': mse_val,\n",
    "                        'throughput_write_mbs': (reference_data.nbytes / (1024**2)) / write_time,\n",
    "                        'throughput_read_mbs': (reference_data.nbytes / (1024**2)) / read_time\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"W:{write_time:.3f}s R:{read_time:.3f}s {ratio:.2f}x\")\n",
    "    \n",
    "    else:\n",
    "        for level in LEVELS:\n",
    "            for chunk_size in CHUNKS:\n",
    "                test_num += 1\n",
    "                print(f\"[{test_num}/{total_tests}] L{level}, c{chunk_size}...\", end=' ')\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                store_path = output_dir / f\"{codec}_l{level}_c{chunk_size}.zarr\"\n",
    "                utils.remove_output_dir(store_path)\n",
    "                chunks = (chunk_size, chunk_size, chunk_size)\n",
    "                \n",
    "                if codec == 'zstd':\n",
    "                    compressor = read_write_zarr.get_zstd_compressor(level)\n",
    "                else:\n",
    "                    compressor = read_write_zarr.get_gzip_compressor(level)\n",
    "                \n",
    "                t0 = time.time()\n",
    "                read_write_zarr.write_zarr_array(\n",
    "                    reference_data, store_path, overwrite=False,\n",
    "                    chunks=chunks, compressor=compressor, zarr_spec=ZARR_SPEC\n",
    "                )\n",
    "                write_time = time.time() - t0\n",
    "                \n",
    "                t0 = time.time()\n",
    "                read_back = read_write_zarr.read_zarr_array(store_path)\n",
    "                read_time = time.time() - t0\n",
    "                \n",
    "                ratio = read_write_zarr.get_compression_ratio(store_path)\n",
    "                size_mb = utils.get_directory_size(store_path) / (1024**2)\n",
    "                ssim_val, psnr_val, mse_val = calculate_metrics(reference_data, read_back)\n",
    "                \n",
    "                all_results.append({\n",
    "                    'codec': codec,\n",
    "                    'shuffle': 'N/A',\n",
    "                    'level': level,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'write_time': write_time,\n",
    "                    'read_time': read_time,\n",
    "                    'ratio': ratio,\n",
    "                    'size_mb': size_mb,\n",
    "                    'ssim': ssim_val,\n",
    "                    'psnr': psnr_val,\n",
    "                    'mse': mse_val,\n",
    "                    'throughput_write_mbs': (reference_data.nbytes / (1024**2)) / write_time,\n",
    "                    'throughput_read_mbs': (reference_data.nbytes / (1024**2)) / read_time\n",
    "                })\n",
    "                \n",
    "                print(f\"W:{write_time:.3f}s R:{read_time:.3f}s {ratio:.2f}x\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚úì Completed {len(all_results)} tests in {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 3. Create Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Add derived metrics\n",
    "df['total_time'] = df['write_time'] + df['read_time']\n",
    "df['efficiency_score'] = df['ratio'] / df['total_time']  # Compression per second\n",
    "df['codec_config'] = df.apply(\n",
    "    lambda x: f\"{x['codec']}_{x['shuffle']}_{x['level']}\" if x['shuffle'] != 'N/A' else f\"{x['codec']}_L{x['level']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = output_dir / f\"compression_matrix_{DOWNLOAD_SIZE}cube.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úì Saved to: {csv_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Total configurations: {len(df)}\")\n",
    "print(f\"  Codecs tested: {df['codec'].nunique()}\")\n",
    "print(f\"  Compression levels: {sorted(df['level'].unique())}\")\n",
    "print(f\"  Chunk sizes: {sorted(df['chunk_size'].unique())}\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matrix-header",
   "metadata": {},
   "source": [
    "## 4. Compression Ratio Matrix (Heat Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ratio-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compression ratio matrix for each chunk size\n",
    "fig, axes = plt.subplots(1, len(CHUNKS), figsize=(6*len(CHUNKS), 6))\n",
    "if len(CHUNKS) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "fig.suptitle('Compression Ratio Matrix by Chunk Size', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, chunk_size in enumerate(CHUNKS):\n",
    "    # Filter data for this chunk size\n",
    "    chunk_df = df[df['chunk_size'] == chunk_size].copy()\n",
    "    \n",
    "    # Create pivot table: codec vs level\n",
    "    # For blosc codecs with shuffle, use shuffle variant as codec name\n",
    "    chunk_df['display_codec'] = chunk_df.apply(\n",
    "        lambda x: f\"{x['codec']}_{x['shuffle'][:3]}\" if 'blosc' in x['codec'] and x['shuffle'] != 'N/A' else x['codec'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    pivot = chunk_df.pivot_table(\n",
    "        values='ratio',\n",
    "        index='display_codec',\n",
    "        columns='level',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='YlGnBu',\n",
    "        cbar_kws={'label': 'Compression Ratio'},\n",
    "        ax=axes[idx],\n",
    "        vmin=1.0,\n",
    "        vmax=df['ratio'].max()\n",
    "    )\n",
    "    \n",
    "    axes[idx].set_title(f'Chunk Size: {chunk_size}¬≥', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Compression Level', fontsize=10)\n",
    "    axes[idx].set_ylabel('Codec', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'compression_ratio_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Compression ratio matrix created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speed-matrix-header",
   "metadata": {},
   "source": [
    "## 5. Read/Write Speed Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speed-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create speed matrices\n",
    "fig, axes = plt.subplots(2, len(CHUNKS), figsize=(6*len(CHUNKS), 12))\n",
    "if len(CHUNKS) == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "fig.suptitle('Read/Write Throughput (MB/s) by Chunk Size', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "for idx, chunk_size in enumerate(CHUNKS):\n",
    "    chunk_df = df[df['chunk_size'] == chunk_size].copy()\n",
    "    chunk_df['display_codec'] = chunk_df.apply(\n",
    "        lambda x: f\"{x['codec']}_{x['shuffle'][:3]}\" if 'blosc' in x['codec'] and x['shuffle'] != 'N/A' else x['codec'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Write throughput\n",
    "    pivot_write = chunk_df.pivot_table(\n",
    "        values='throughput_write_mbs',\n",
    "        index='display_codec',\n",
    "        columns='level',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_write,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='RdYlGn',\n",
    "        cbar_kws={'label': 'Write Throughput (MB/s)'},\n",
    "        ax=axes[0, idx]\n",
    "    )\n",
    "    \n",
    "    axes[0, idx].set_title(f'Write: Chunk {chunk_size}¬≥', fontsize=11, fontweight='bold')\n",
    "    axes[0, idx].set_xlabel('Compression Level', fontsize=9)\n",
    "    axes[0, idx].set_ylabel('Codec', fontsize=9)\n",
    "    \n",
    "    # Read throughput\n",
    "    pivot_read = chunk_df.pivot_table(\n",
    "        values='throughput_read_mbs',\n",
    "        index='display_codec',\n",
    "        columns='level',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_read,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='RdYlGn',\n",
    "        cbar_kws={'label': 'Read Throughput (MB/s)'},\n",
    "        ax=axes[1, idx]\n",
    "    )\n",
    "    \n",
    "    axes[1, idx].set_title(f'Read: Chunk {chunk_size}¬≥', fontsize=11, fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Compression Level', fontsize=9)\n",
    "    axes[1, idx].set_ylabel('Codec', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'speed_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Speed matrices created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoff-header",
   "metadata": {},
   "source": [
    "## 6. Compression vs Speed Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Compression ratio vs Total time, colored by codec\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Compression vs Write time\n",
    "for codec in df['codec'].unique():\n",
    "    codec_df = df[df['codec'] == codec]\n",
    "    axes[0].scatter(\n",
    "        codec_df['write_time'],\n",
    "        codec_df['ratio'],\n",
    "        label=codec,\n",
    "        alpha=0.6,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel('Write Time (s)', fontsize=12)\n",
    "axes[0].set_ylabel('Compression Ratio', fontsize=12)\n",
    "axes[0].set_title('Compression Ratio vs Write Speed', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Compression vs Read time\n",
    "for codec in df['codec'].unique():\n",
    "    codec_df = df[df['codec'] == codec]\n",
    "    axes[1].scatter(\n",
    "        codec_df['read_time'],\n",
    "        codec_df['ratio'],\n",
    "        label=codec,\n",
    "        alpha=0.6,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "axes[1].set_xlabel('Read Time (s)', fontsize=12)\n",
    "axes[1].set_ylabel('Compression Ratio', fontsize=12)\n",
    "axes[1].set_title('Compression Ratio vs Read Speed', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'compression_speed_tradeoff.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Trade-off analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto-header",
   "metadata": {},
   "source": [
    "## 7. Pareto Frontier Analysis\n",
    "\n",
    "Find optimal configurations on the Pareto frontier (best compression for given speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Pareto frontier for compression vs total time\n",
    "def is_pareto_efficient(costs):\n",
    "    \"\"\"\n",
    "    Find Pareto efficient points\n",
    "    costs: (n_points, n_costs) array\n",
    "    \"\"\"\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            # Keep points that are not dominated\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n",
    "            is_efficient[i] = True\n",
    "    return is_efficient\n",
    "\n",
    "# Prepare data for Pareto analysis\n",
    "# We want to minimize time and maximize ratio\n",
    "# So we use (time, -ratio) as costs\n",
    "costs = np.column_stack([\n",
    "    df['total_time'].values,\n",
    "    -df['ratio'].values  # Negative because we want to maximize\n",
    "])\n",
    "\n",
    "pareto_mask = is_pareto_efficient(costs)\n",
    "pareto_df = df[pareto_mask].copy()\n",
    "pareto_df = pareto_df.sort_values('total_time')\n",
    "\n",
    "# Visualize Pareto frontier\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot all points\n",
    "ax.scatter(\n",
    "    df['total_time'],\n",
    "    df['ratio'],\n",
    "    alpha=0.3,\n",
    "    s=50,\n",
    "    label='All configurations',\n",
    "    color='gray'\n",
    ")\n",
    "\n",
    "# Plot Pareto frontier\n",
    "ax.scatter(\n",
    "    pareto_df['total_time'],\n",
    "    pareto_df['ratio'],\n",
    "    alpha=0.8,\n",
    "    s=200,\n",
    "    label='Pareto optimal',\n",
    "    color='red',\n",
    "    edgecolors='black',\n",
    "    linewidths=2\n",
    ")\n",
    "\n",
    "# Connect Pareto points\n",
    "ax.plot(\n",
    "    pareto_df['total_time'],\n",
    "    pareto_df['ratio'],\n",
    "    'r--',\n",
    "    alpha=0.5,\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Annotate Pareto points\n",
    "for _, row in pareto_df.iterrows():\n",
    "    label = f\"{row['codec']}\\nL{row['level']}, c{row['chunk_size']}\"\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (row['total_time'], row['ratio']),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0')\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Total Time (Write + Read) [s]', fontsize=12)\n",
    "ax.set_ylabel('Compression Ratio', fontsize=12)\n",
    "ax.set_title('Pareto Frontier: Optimal Compression vs Speed Trade-offs', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'pareto_frontier.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Found {len(pareto_df)} Pareto optimal configurations\")\n",
    "print(\"\\nPareto Optimal Configurations:\")\n",
    "print(pareto_df[['codec', 'shuffle', 'level', 'chunk_size', 'total_time', 'ratio', 'size_mb']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPRESSION MATRIX SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset: {DOWNLOAD_SIZE}¬≥ CryoET tomogram ({(DOWNLOAD_SIZE**3 * 4)/(1024**2):.1f} MB)\")\n",
    "print(f\"Configurations tested: {len(df)}\")\n",
    "print(f\"Total benchmark time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. BEST OVERALL PERFORMERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best compression\n",
    "best_comp = df.loc[df['ratio'].idxmax()]\n",
    "print(f\"\\nüèÜ Best Compression Ratio: {best_comp['ratio']:.3f}√ó\")\n",
    "print(f\"   Codec: {best_comp['codec']} (shuffle={best_comp['shuffle']}, level={best_comp['level']}, chunk={best_comp['chunk_size']})\")\n",
    "print(f\"   Size: {best_comp['size_mb']:.2f} MB (saves {64 - best_comp['size_mb']:.2f} MB)\")\n",
    "print(f\"   Time: Write={best_comp['write_time']:.3f}s, Read={best_comp['read_time']:.3f}s\")\n",
    "\n",
    "# Fastest write\n",
    "best_write = df.loc[df['write_time'].idxmin()]\n",
    "print(f\"\\n‚ö° Fastest Write: {best_write['write_time']:.3f}s ({best_write['throughput_write_mbs']:.1f} MB/s)\")\n",
    "print(f\"   Codec: {best_write['codec']} (level={best_write['level']}, chunk={best_write['chunk_size']})\")\n",
    "print(f\"   Compression: {best_write['ratio']:.3f}√ó\")\n",
    "\n",
    "# Fastest read\n",
    "best_read = df.loc[df['read_time'].idxmin()]\n",
    "print(f\"\\n‚ö° Fastest Read: {best_read['read_time']:.3f}s ({best_read['throughput_read_mbs']:.1f} MB/s)\")\n",
    "print(f\"   Codec: {best_read['codec']} (level={best_read['level']}, chunk={best_read['chunk_size']})\")\n",
    "print(f\"   Compression: {best_read['ratio']:.3f}√ó\")\n",
    "\n",
    "# Best efficiency\n",
    "best_eff = df.loc[df['efficiency_score'].idxmax()]\n",
    "print(f\"\\n‚öôÔ∏è  Best Efficiency (compression/time): {best_eff['efficiency_score']:.3f}\")\n",
    "print(f\"   Codec: {best_eff['codec']} (shuffle={best_eff['shuffle']}, level={best_eff['level']}, chunk={best_eff['chunk_size']})\")\n",
    "print(f\"   Ratio: {best_eff['ratio']:.3f}√ó, Time: {best_eff['total_time']:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. CODEC COMPARISON (Chunk=128, Level=5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = df[(df['chunk_size']==128) & (df['level'].isin([5, 0]))].copy()\n",
    "comparison_df = comparison_df.sort_values('ratio', ascending=False)\n",
    "\n",
    "print(f\"\\n{'Codec':<25} {'Shuffle':<12} {'Ratio':<8} {'Write(s)':<10} {'Read(s)':<10} {'Size(MB)':<10}\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "for _, row in comparison_df.head(10).iterrows():\n",
    "    codec_name = f\"{row['codec']}\"\n",
    "    shuffle_str = row['shuffle'] if row['shuffle'] != 'N/A' else '-'\n",
    "    print(f\"{codec_name:<25} {shuffle_str:<12} {row['ratio']:<8.3f} {row['write_time']:<10.3f} {row['read_time']:<10.3f} {row['size_mb']:<10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. CHUNK SIZE IMPACT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for chunk in CHUNKS:\n",
    "    chunk_subset = df[df['chunk_size'] == chunk]\n",
    "    print(f\"\\nChunk size {chunk}¬≥:\")\n",
    "    print(f\"  Avg compression ratio: {chunk_subset['ratio'].mean():.3f}√ó\")\n",
    "    print(f\"  Avg write time: {chunk_subset['write_time'].mean():.3f}s\")\n",
    "    print(f\"  Avg read time: {chunk_subset['read_time'].mean():.3f}s\")\n",
    "    print(f\"  Best ratio: {chunk_subset['ratio'].max():.3f}√ó ({chunk_subset.loc[chunk_subset['ratio'].idxmax(), 'codec']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. RECOMMENDATIONS BY USE CASE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüì¶ For Long-term Archival (Best Compression):\")\n",
    "print(f\"   ‚Üí {best_comp['codec']} with {best_comp['shuffle']} shuffle, level {best_comp['level']}, chunk {best_comp['chunk_size']}¬≥\")\n",
    "print(f\"   ‚Üí Saves {((1 - best_comp['size_mb']/64) * 100):.1f}% storage space\")\n",
    "\n",
    "print(\"\\n‚ö° For Real-time Processing (Best Speed):\")\n",
    "fastest_overall = df.loc[df['total_time'].idxmin()]\n",
    "print(f\"   ‚Üí {fastest_overall['codec']} level {fastest_overall['level']}, chunk {fastest_overall['chunk_size']}¬≥\")\n",
    "print(f\"   ‚Üí Total time: {fastest_overall['total_time']:.3f}s (write + read)\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  For General Use (Best Balance):\")\n",
    "# Find best balance: good compression (>1.1x) and fast (<0.1s total)\n",
    "balanced = df[(df['ratio'] > 1.1) & (df['total_time'] < 0.1)]\n",
    "if not balanced.empty:\n",
    "    best_balanced = balanced.loc[balanced['efficiency_score'].idxmax()]\n",
    "    print(f\"   ‚Üí {best_balanced['codec']} level {best_balanced['level']}, chunk {best_balanced['chunk_size']}¬≥\")\n",
    "    print(f\"   ‚Üí Ratio: {best_balanced['ratio']:.3f}√ó, Time: {best_balanced['total_time']:.3f}s\")\n",
    "else:\n",
    "    print(f\"   ‚Üí {best_eff['codec']} level {best_eff['level']}, chunk {best_eff['chunk_size']}¬≥\")\n",
    "    print(f\"   ‚Üí Efficiency score: {best_eff['efficiency_score']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ COMPREHENSIVE MATRIX ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_dir}\")\n",
    "print(\"Files generated:\")\n",
    "print(f\"  - compression_matrix_{DOWNLOAD_SIZE}cube.csv\")\n",
    "print(\"  - compression_ratio_matrix.png\")\n",
    "print(\"  - speed_matrices.png\")\n",
    "print(\"  - compression_speed_tradeoff.png\")\n",
    "print(\"  - pareto_frontier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 9. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown summary report\n",
    "report = f\"\"\"# Compression Benchmarking Matrix Report\n",
    "\n",
    "**Dataset:** CryoET Tomogram {DOWNLOAD_SIZE}¬≥ ({(DOWNLOAD_SIZE**3 * 4)/(1024**2):.1f} MB)\n",
    "**Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Configurations Tested:** {len(df)}\n",
    "**Benchmark Time:** {total_time/60:.1f} minutes\n",
    "\n",
    "## Best Performers\n",
    "\n",
    "### üèÜ Best Compression\n",
    "- **Codec:** {best_comp['codec']} (shuffle={best_comp['shuffle']}, level={best_comp['level']}, chunk={best_comp['chunk_size']}¬≥)\n",
    "- **Ratio:** {best_comp['ratio']:.3f}√ó\n",
    "- **Size:** {best_comp['size_mb']:.2f} MB (saves {64 - best_comp['size_mb']:.2f} MB, {((1 - best_comp['size_mb']/64) * 100):.1f}%)\n",
    "- **Time:** Write={best_comp['write_time']:.3f}s, Read={best_comp['read_time']:.3f}s\n",
    "\n",
    "### ‚ö° Fastest Write\n",
    "- **Codec:** {best_write['codec']} (level={best_write['level']}, chunk={best_write['chunk_size']}¬≥)\n",
    "- **Time:** {best_write['write_time']:.3f}s ({best_write['throughput_write_mbs']:.1f} MB/s)\n",
    "- **Ratio:** {best_write['ratio']:.3f}√ó\n",
    "\n",
    "### ‚ö° Fastest Read\n",
    "- **Codec:** {best_read['codec']} (level={best_read['level']}, chunk={best_read['chunk_size']}¬≥)\n",
    "- **Time:** {best_read['read_time']:.3f}s ({best_read['throughput_read_mbs']:.1f} MB/s)\n",
    "- **Ratio:** {best_read['ratio']:.3f}√ó\n",
    "\n",
    "## Pareto Optimal Configurations\n",
    "\n",
    "{pareto_df[['codec', 'shuffle', 'level', 'chunk_size', 'ratio', 'total_time']].to_markdown(index=False)}\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "**For Archival Storage:**\n",
    "- Use: {best_comp['codec']} with {best_comp['shuffle']} shuffle, level {best_comp['level']}, chunk {best_comp['chunk_size']}¬≥\n",
    "- Benefit: {((1 - best_comp['size_mb']/64) * 100):.1f}% storage savings\n",
    "\n",
    "**For Real-time Processing:**\n",
    "- Use: {fastest_overall['codec']} level {fastest_overall['level']}, chunk {fastest_overall['chunk_size']}¬≥\n",
    "- Benefit: {fastest_overall['total_time']:.3f}s total latency\n",
    "\n",
    "**For General Use:**\n",
    "- Use: {best_eff['codec']} level {best_eff['level']}, chunk {best_eff['chunk_size']}¬≥\n",
    "- Benefit: Best efficiency score ({best_eff['efficiency_score']:.3f})\n",
    "\"\"\"\n",
    "\n",
    "report_path = output_dir / f\"compression_matrix_report_{DOWNLOAD_SIZE}cube.md\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"‚úì Summary report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**To test different data:**\n",
    "1. Change `DOWNLOAD_SIZE` to test on larger volumes\n",
    "2. Change dataset ID to test on different tomograms\n",
    "\n",
    "**To expand the matrix:**\n",
    "1. Set `FULL_MATRIX = True` for complete testing\n",
    "2. Add custom codec configurations\n",
    "\n",
    "**Analysis ideas:**\n",
    "- Compare results across multiple datasets\n",
    "- Test with different data types (segmentations, etc.)\n",
    "- Analyze impact of data characteristics on compression\n",
    "\n",
    "**Resources:**\n",
    "- CryoET Portal: https://cryoetdataportal.czscience.com/\n",
    "- Zarr Docs: https://zarr.readthedocs.io/\n",
    "- Numcodecs: https://numcodecs.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (zarr-benchmarks)",
   "language": "python",
   "name": "zarr-benchmarks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
